{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a1ab460-b68e-4731-94da-203eaad75b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime\n",
    "import time\n",
    "from onnxruntime.quantization import QuantFormat, QuantType, quantize_static\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37cd101-5574-48af-9ceb-432d4477509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxruntime.quantization.preprocess --input ..\\data\\models\\tha3\\standard\\fp32\\decomposer.onnx --output .\\preprocessed\\decomposer.onnx\n",
    "!python -m onnxruntime.quantization.preprocess --input ..\\data\\models\\tha3\\standard\\fp32\\combiner.onnx --output .\\preprocessed\\combiner.onnx\n",
    "!python -m onnxruntime.quantization.preprocess --input ..\\data\\models\\tha3\\standard\\fp32\\morpher.onnx --output .\\preprocessed\\morpher.onnx\n",
    "!python -m onnxruntime.quantization.preprocess --input ..\\data\\models\\tha3\\standard\\fp32\\rotator.onnx --output .\\preprocessed\\rotator.onnx\n",
    "!python -m onnxruntime.quantization.preprocess --input ..\\data\\models\\tha3\\standard\\fp32\\editor.onnx --output .\\preprocessed\\editor.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687242ed-630e-4092-93d9-1df97b224e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import CalibrationDataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c114c0d5-e540-4003-a2aa-88b3956ec603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_images(images_folder: str):\n",
    "    \"\"\"\n",
    "    Loads a batch of images and preprocess them\n",
    "    parameter images_folder: path to folder storing images\n",
    "    parameter height: image height in pixels\n",
    "    parameter width: image width in pixels\n",
    "    parameter size_limit: number of images to load. Default is 0 which means all images are picked.\n",
    "    return: list of matrices characterizing multiple images\n",
    "    \"\"\"\n",
    "    batch_filenames = os.listdir(images_folder)\n",
    "    batch_data = []\n",
    "    for image_name in tqdm(batch_filenames):\n",
    "        image_filepath = os.path.join(images_folder, image_name)\n",
    "        img = cv2.imread(image_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (450,900),interpolation = cv2.INTER_LANCZOS4)\n",
    "        padding_img = np.zeros((1024,1024,4),np.uint8)\n",
    "        padding_img[40:40+900,287:287+450,:] = img\n",
    "        img = cv2.resize(padding_img, (512,512),interpolation = cv2.INTER_LANCZOS4)\n",
    "        batch_data.append(img)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db0fda0-3262-46db-be6e-1e10048b87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposerDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, None)\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.data_list = _preprocess_images(\n",
    "            calibration_image_folder\n",
    "        )\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.data_list)\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: data} for data in self.data_list]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb72686-f661-4906-b36b-491b841b6a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 298/298 [00:20<00:00, 14.67it/s]\n"
     ]
    }
   ],
   "source": [
    "dr = DecomposerDataReader('Z:/ComfyUI-aki-v1.5/output/', 'C:/EasyVtuber/data/models/tha3/standard/fp32/decomposer.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7580fbaf-fffd-4c54-a486-05070ae47de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.downsample_blocks.0.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.downsample_blocks.1.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.downsample_blocks.2.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.downsample_blocks.3.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.0.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.1.resnet_path.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.1.resnet_path.4.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.2.resnet_path.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.2.resnet_path.4.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.3.resnet_path.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.3.resnet_path.4.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.4.resnet_path.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.4.resnet_path.4.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.5.resnet_path.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.bottleneck_blocks.5.resnet_path.4.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.upsample_blocks.0.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.upsample_blocks.1.1.weight' with rank 1\n",
      "WARNING:root:Axis 1 is out-of-range for weight 'eyebrow_decomposer.body.upsample_blocks.2.1.weight' with rank 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArgMax', 'Softmax', 'Unsqueeze', 'Clip', 'Add', 'Transpose', 'EmbedLayerNormalization', 'LayerNormalization', 'Split', 'Pad', 'Slice', 'GatherElements', 'MatMul', 'Reshape', 'BatchNormalization', 'Conv', 'Relu', 'InstanceNormalization', 'Gather', 'Sigmoid', 'LeakyRelu', 'MaxPool', 'Gemm', 'Where', 'Resize', 'AveragePool', 'Squeeze', 'Mul', 'Concat', 'ConvTranspose', 'GlobalAveragePool']\n",
      "\n",
      "com.microsoft.nchwc\n",
      "ai.onnx.ml\n",
      "ai.onnx.training\n",
      "ai.onnx.preview.training\n",
      "com.microsoft\n",
      "com.microsoft.experimental\n",
      "org.pytorch.aten\n",
      "com.microsoft.dml\n",
      "[domain: \"\"\n",
      "version: 16\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "quantize_static(\n",
    "        './preprocessed/decomposer.onnx',\n",
    "        './quantized/decomposer.onnx',\n",
    "        dr,\n",
    "        quant_format=QuantFormat.QDQ,\n",
    "        per_channel=True,\n",
    "        weight_type=QuantType.QInt8,\n",
    "        nodes_to_exclude = ['/Mul_3','/Add'],\n",
    "        extra_options = {\n",
    "            'ActivationSymmetric':True,\n",
    "            'QuantizeBias':False\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e8aad5-434e-44de-a4f7-d8391cd42e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_quantized_session =onnxruntime.InferenceSession('./preprocessed/decomposer.onnx', None)\n",
    "quantized_session =onnxruntime.InferenceSession('./quantized/decomposer.onnx', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b731eb9-a9db-492f-9856-fdbabd5829f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.00043852476)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.rewind()\n",
    "inp = dr.get_next()\n",
    "non_res =  non_quantized_session.run(None, inp)\n",
    "qt_res = quantized_session.run(None, inp)\n",
    "((qt_res[1] -non_res[1])**2).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
